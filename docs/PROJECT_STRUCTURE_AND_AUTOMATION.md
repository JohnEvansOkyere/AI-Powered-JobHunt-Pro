# Project Structure & Automation Guide

## ğŸ“ Organized Project Structure

```
AI-Powered-JobHunt-Pro/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ ai/                 # AI providers (OpenAI, Grok, Gemini)
â”‚   â”‚   â”œâ”€â”€ api/                # API endpoints
â”‚   â”‚   â”œâ”€â”€ core/               # Core configs (database, logging, settings)
â”‚   â”‚   â”œâ”€â”€ exceptions/         # Custom exceptions
â”‚   â”‚   â”œâ”€â”€ middleware/         # Error handling, request logging
â”‚   â”‚   â”œâ”€â”€ models/             # Database models (Job, User, CV, etc.)
â”‚   â”‚   â”œâ”€â”€ scrapers/           # Job board scrapers (RemoteOK, SerpAPI, etc.)
â”‚   â”‚   â”œâ”€â”€ services/           # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ ai_job_matcher.py          # ğŸ¤– AI-powered job matching
â”‚   â”‚   â”‚   â”œâ”€â”€ job_matching_service.py    # Original keyword matching
â”‚   â”‚   â”‚   â””â”€â”€ job_matching_service_optimized.py  # Optimized keyword matching
â”‚   â”‚   â””â”€â”€ tasks/              # Background tasks (Celery)
â”‚   â”œâ”€â”€ scripts/                # ğŸ“œ All scripts belong here
â”‚   â”‚   â”œâ”€â”€ cleanup_old_jobs.py           # Archive/delete old jobs
â”‚   â”‚   â”œâ”€â”€ daily_job_scraper.py          # Daily automated job scraping
â”‚   â”‚   â”œâ”€â”€ setup_cron_jobs.sh            # Setup automation (cron)
â”‚   â”‚   â”œâ”€â”€ seed_jobs.py                  # Original seeding script
â”‚   â”‚   â”œâ”€â”€ seed_jobs_remoteok.py         # RemoteOK seeding
â”‚   â”‚   â”œâ”€â”€ seed_jobs_improved.py         # Multi-query seeding
â”‚   â”‚   â””â”€â”€ seed_jobs_simple.py           # Simple seeding
â”‚   â”œâ”€â”€ tests/                  # Test files
â”‚   â”œâ”€â”€ .env                    # Environment variables
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ main.py                 # FastAPI entry point
â”‚
â”œâ”€â”€ frontend/                   # Next.js frontend
â”‚   â”œâ”€â”€ app/                    # Next.js 13+ app router
â”‚   â”œâ”€â”€ components/             # React components
â”‚   â”œâ”€â”€ lib/                    # Utilities and API clients
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ docs/                       # ğŸ“š All documentation belongs here
    â”œâ”€â”€ AI_MATCHING_UPGRADE.md               # AI matching implementation
    â”œâ”€â”€ JOB_MATCHING_FIX.md                  # Job matching fixes
    â”œâ”€â”€ SEEDING_GUIDE.md                     # Job seeding guide
    â”œâ”€â”€ TESTING_QUICKSTART.md                # Testing guide
    â”œâ”€â”€ FRONTEND_BACKEND_DIAGNOSTICS.md      # Connection diagnostics
    â”œâ”€â”€ TESTING_SETUP_SUMMARY.md             # Testing setup
    â”œâ”€â”€ MULTI_TITLE_FEATURE.md               # Multi-title job search feature
    â”œâ”€â”€ PROFILE_SIMPLIFICATION.md            # Profile wizard simplification (6â†’5 steps)
    â””â”€â”€ PROJECT_STRUCTURE_AND_AUTOMATION.md  # This file
```

## ğŸ¤– AI-Powered Job Matching

### What It Does
Matches jobs to user profiles using **OpenAI embeddings** (semantic similarity), not just keyword matching.

### Location
- Service: `backend/app/services/ai_job_matcher.py`
- Used in: `backend/app/api/v1/endpoints/jobs.py`

### How It Works
1. Creates AI embedding of user profile (skills, experience, goals)
2. Creates AI embedding of each job (title, description)
3. Calculates similarity score (0-100%)
4. Shows only 50%+ matches (motivating quality)

### Performance
- First search: 3-5 seconds
- Cached: <100ms
- Cost: ~$0.0004 per search

## ğŸ“… Automated Job Management

### 1. Daily Job Scraping
**Script**: `scripts/daily_job_scraper.py`

**What it does**:
- Scrapes RemoteOK (free, no API key)
- Scrapes SerpAPI (requires `SERPAPI_API_KEY` in `.env`)
- Removes duplicates
- Adds ~50-100 new jobs daily

**Run manually**:
```bash
cd backend
python scripts/daily_job_scraper.py
```

### 2. Database Cleanup (Application-Aware)
**Script**: `scripts/cleanup_old_jobs.py`

**What it does**:
- **KEEPS** jobs that users have applied to (forever - for tracking)
- **DELETES** jobs older than 7 days with NO applications
- Cleans old job match cache (90+ days)

**Run manually**:
```bash
cd backend
python scripts/cleanup_old_jobs.py
```

**Why this strategy?**
- âœ… Users can track their application history forever
- âœ… Database stays lean (removes jobs nobody cared about)
- âœ… Never loses jobs users actually applied to

### 3. Setup Automation (Cron)
**Script**: `scripts/setup_cron_jobs.sh`

**What it does**:
- Sets up daily cron jobs for scraping (2 AM) and cleanup (3 AM)
- Creates log files in `backend/logs/`

**Setup**:
```bash
cd backend
chmod +x scripts/setup_cron_jobs.sh
./scripts/setup_cron_jobs.sh
```

**View scheduled jobs**:
```bash
crontab -l
```

**View logs**:
```bash
tail -f logs/job_scraper.log
tail -f logs/job_cleanup.log
```

## ğŸ”§ Database Management Strategy (Application-Aware)

### Job Lifecycle

**For jobs WITH applications** (user applied):
```
Day 0:   Job scraped
Day 1:   User applies to job â†’ Application created
Day 7+:  Job KEPT forever (for user's application tracking)
```

**For jobs WITHOUT applications** (user didn't care):
```
Day 0:  Job scraped â†’ status: "pending"
Day 1:  Matched to user â†’ Cached in job_matches
Day 7:  No applications â†’ DELETED from database
```

### Why Application-Aware Cleanup?

**Jobs WITH applications:**
- âœ… Kept **forever** so users can track their applications
- âœ… Preserves application history
- âœ… Users can reference job details months later

**Jobs WITHOUT applications:**
- âœ… Deleted after 7 days (nobody cared about them)
- âœ… Keeps database lean and fast
- âœ… Removes noise from job searches

### Database Tables
- `jobs` - Job postings (kept if has applications, deleted after 7 days if not)
- `applications` - User's job applications (kept forever)
- `job_matches` - Cached AI match results (cleaned after 90 days)
- User sees only jobs with 50%+ match scores

## ğŸš€ Quick Start

### Initial Setup
```bash
# 1. Seed some jobs
cd backend
python scripts/seed_jobs_remoteok.py

# 2. Setup automated scraping
./scripts/setup_cron_jobs.sh

# 3. Start backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### User searches for jobs
1. Frontend calls `/api/v1/jobs/?matched=true`
2. Backend uses AI matcher (50%+ threshold)
3. Returns cached matches if available (<1 hour old)
4. Otherwise computes new matches using OpenAI embeddings
5. Caches results for 1 hour

## ğŸ“Š Monitoring

### Check Database Stats
```bash
cd backend
python scripts/cleanup_old_jobs.py  # Shows stats at end
```

### Check API Logs
```bash
# Backend logs
tail -f backend.log | grep "AI"

# Scraping logs
tail -f logs/job_scraper.log

# Cleanup logs
tail -f logs/job_cleanup.log
```

### Check Cron Jobs
```bash
crontab -l
```

## ğŸ”‘ Environment Variables

Required in `backend/.env`:
```bash
# OpenAI (for AI job matching)
OPENAI_API_KEY=sk-proj-...

# SerpAPI (optional, for Google Jobs scraping)
SERPAPI_API_KEY=...

# Supabase (database)
DATABASE_URL=postgresql://...
SUPABASE_URL=https://...
SUPABASE_KEY=...
SUPABASE_SERVICE_KEY=...
```

## ğŸ¯ Key Features

### AI Job Matching
- âœ… 50%+ match scores (motivating quality)
- âœ… Understands synonyms and context
- âœ… Fast with 1-hour caching
- âœ… Cost-effective (~$0.0004/search)

### Automated Job Scraping
- âœ… Daily scraping from RemoteOK + SerpAPI
- âœ… Duplicate detection
- âœ… Auto-cleanup of old jobs
- âœ… Logs all activity

### Database Management
- âœ… Smart archiving (not deletion)
- âœ… 7-day threshold for archiving
- âœ… 30-day threshold for deletion
- âœ… Keeps database lean and fast

## ğŸ“ Next Steps

1. **Monitor costs**: Check OpenAI API usage dashboard
2. **Tune threshold**: Adjust MIN_SCORE in `ai_job_matcher.py` if needed
3. **Add more sources**: Extend `daily_job_scraper.py` with more job boards
4. **User feedback**: Let users mark jobs as "not relevant" to improve matching

## ğŸ†˜ Troubleshooting

### No jobs showing up?
```bash
# Check if jobs exist
cd backend
python scripts/seed_jobs_remoteok.py

# Check if AI matcher is working
tail -f backend.log | grep "AI"
```

### Cron not running?
```bash
# Check cron jobs
crontab -l

# Check logs
ls -la logs/
```

### High API costs?
- Reduce `MAX_JOBS_PER_REQUEST` in `ai_job_matcher.py`
- Increase `CACHE_EXPIRY_HOURS` to cache longer
- Use keyword matching instead of AI for some searches

---

**All code is now properly organized!** ğŸ‰
- Scripts â†’ `backend/scripts/`
- Docs â†’ `docs/`
- Services â†’ `backend/app/services/`
