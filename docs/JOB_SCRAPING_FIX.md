# Job Scraping Issue - Root Cause and Solution

## Problem Identified ‚úÖ

**Why candidates aren't getting job matches:**

The job matching system is **fully functional**, but there are **NO JOBS IN THE DATABASE** to match against. This is because:

1. **2 out of 5 scrapers are placeholders** (LinkedIn, Indeed) - they return empty lists
2. **2 scrapers work** (Remotive, SerpAPI) but haven't been triggered yet
3. **No initial scraping has been run** to populate the database
4. **Database `jobs` table is empty** on fresh installation

## Root Cause Analysis

### What's Working ‚úÖ
- ‚úÖ Job matching algorithm (fully implemented)
- ‚úÖ Remotive scraper (works, no API key needed)
- ‚úÖ SerpAPI scraper (works, requires API key)
- ‚úÖ API endpoints (all functional)
- ‚úÖ Background tasks (Celery configured)
- ‚úÖ Database models (properly defined)
- ‚úÖ Frontend job display (ready to show jobs)

### What's Broken ‚ùå
- ‚ùå LinkedIn scraper - **PLACEHOLDER** (returns empty list)
- ‚ùå Indeed scraper - **PLACEHOLDER** (returns empty list)
- ‚ùå **NO INITIAL JOBS** in database (root cause)
- ‚ùå Missing `.env` typo: `ERPAPI_API_KEY` should be `SERPAPI_API_KEY`

### Evidence

**LinkedIn Scraper** (`backend/app/scrapers/linkedin_scraper.py:62-63`):
```python
logger.warning("LinkedIn scraper not fully implemented. Returning empty results.")
return []
```

**Indeed Scraper** (`backend/app/scrapers/indeed_scraper.py:64-65`):
```python
logger.warning("Indeed scraper not fully implemented. Returning empty results.")
return []
```

**Job Matching Logic** (`backend/app/services/job_matching_service.py:88-91`):
```python
if not jobs:
    logger.warning(f"No jobs available for matching for user {user_id}")
    return []
```

## Immediate Solutions

### Solution 1: Use Remotive (Recommended - No API Key Needed)

**Step 1: Start Celery Worker**
```bash
cd /home/grejoy/Projects/AI-Powered-JobHunt-Pro/backend

# Activate venv if you have one
source venv/bin/activate

# Start Celery worker
celery -A app.tasks.celery_app worker --loglevel=info
```

**Step 2: Trigger Scraping**

Option A - Via API (requires authentication):
```bash
curl -X POST http://localhost:8000/api/v1/jobs/scrape \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "sources": ["remotive"],
    "keywords": ["python", "developer", "engineer"],
    "location": "remote",
    "max_results_per_source": 100
  }'
```

Option B - Via Python Script:
```python
# Create: backend/seed_jobs.py
import asyncio
import sys
from pathlib import Path

# Add backend to path
sys.path.insert(0, str(Path(__file__).parent))

from app.core.config import settings
from app.services.job_scraper_service import JobScraperService
from app.core.database import SessionLocal

async def seed_jobs():
    """Seed initial jobs from Remotive."""
    db = SessionLocal()
    try:
        scraper_service = JobScraperService()

        print("üîÑ Scraping jobs from Remotive...")
        results = await scraper_service.scrape_jobs(
            sources=["remotive"],
            keywords=["python", "developer", "engineer", "data"],
            location="remote",
            max_results_per_source=100
        )

        print(f"‚úÖ Scraped {len(results)} jobs from Remotive")

        # Save to database
        from app.services.job_processor import JobProcessor
        processor = JobProcessor()

        for job_data in results:
            await processor.process_job(job_data, db)

        print(f"‚úÖ Saved jobs to database")

    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(seed_jobs())
```

Run:
```bash
cd /home/grejoy/Projects/AI-Powered-JobHunt-Pro/backend
python seed_jobs.py
```

### Solution 2: Use SerpAPI (Requires API Key)

**Step 1: Fix `.env` Typo**
```bash
cd /home/grejoy/Projects/AI-Powered-JobHunt-Pro/backend

# Edit .env file
# Change: ERPAPI_API_KEY=...
# To:     SERPAPI_API_KEY=...
```

**Current `.env` has:**
```env
ERPAPI_API_KEY=3f216dd7e001789035383df59c4af6a1dcb8d77ea43a2ef566ab87b68d0b17a3
```

**Should be:**
```env
SERPAPI_API_KEY=3f216dd7e001789035383df59c4af6a1dcb8d77ea43a2ef566ab87b68d0b17a3
```

**Step 2: Restart Backend**
```bash
# Kill existing backend process
pkill -f uvicorn

# Restart
cd /home/grejoy/Projects/AI-Powered-JobHunt-Pro/backend
uvicorn app.main:app --reload
```

**Step 3: Trigger Scraping with SerpAPI**
```bash
curl -X POST http://localhost:8000/api/v1/jobs/scrape \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "sources": ["serpapi", "remotive"],
    "keywords": ["software engineer"],
    "location": "Ghana",
    "max_results_per_source": 50
  }'
```

### Solution 3: Automated Periodic Scraping

**Create Celery Beat Schedule** (`backend/app/tasks/celery_app.py`):
```python
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    'scrape-jobs-daily': {
        'task': 'app.tasks.job_scraping.scrape_jobs_task',
        'schedule': crontab(hour=2, minute=0),  # Run at 2 AM daily
        'args': (
            ["remotive", "serpapi"],  # sources
            ["python", "developer", "engineer"],  # keywords
            "remote",  # location
            100,  # max_results_per_source
        ),
    },
}
```

**Start Celery Beat:**
```bash
celery -A app.tasks.celery_app beat --loglevel=info
```

## Verification Steps

### 1. Check Scraping Status
```bash
curl http://localhost:8000/api/v1/jobs/scraping/ \
  -H "Authorization: Bearer YOUR_TOKEN"
```

Expected response:
```json
{
  "scraping_jobs": [
    {
      "id": "uuid-here",
      "status": "completed",
      "progress": 100,
      "jobs_found": 50
    }
  ]
}
```

### 2. Verify Jobs in Database

Via Supabase SQL Editor:
```sql
-- Count total jobs
SELECT COUNT(*) FROM jobs;

-- See recent jobs
SELECT title, company, location, created_at
FROM jobs
ORDER BY created_at DESC
LIMIT 10;

-- Check processing status
SELECT processing_status, COUNT(*)
FROM jobs
GROUP BY processing_status;
```

### 3. Test Job Search API
```bash
curl http://localhost:8000/api/v1/jobs/ \
  -H "Authorization: Bearer YOUR_TOKEN"
```

Expected:
```json
{
  "jobs": [...],
  "total": 50,
  "page": 1,
  "page_size": 20
}
```

### 4. Test Job Matching
```bash
curl "http://localhost:8000/api/v1/jobs/?matched=true" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

Expected:
```json
{
  "jobs": [
    {
      "id": "...",
      "title": "Python Developer",
      "match_score": 0.85,
      "match_reasons": ["Skills match: Python, FastAPI"]
    }
  ]
}
```

## Frontend Integration

The frontend is already configured to display jobs. Once jobs are in the database, they will automatically appear.

**Current Frontend Request** (`frontend/app/dashboard/jobs/page.tsx:41`):
```typescript
const params: JobSearchParams = {
  page,
  page_size: 20,
  matched: true, // Get matched jobs with scores
}

const data = await searchJobs(params)
```

## Long-term Improvements

### 1. Implement LinkedIn/Indeed Scrapers
These are currently placeholders. Options:
- Use official APIs (LinkedIn API, Indeed API)
- Implement web scraping (respect robots.txt, rate limits)
- Use third-party services (ScraperAPI, Apify)

### 2. Add More Job Sources
- RemoteOK
- We Work Remotely
- AngelList
- GitHub Jobs (deprecated, find alternative)
- Stack Overflow Jobs

### 3. Add UI for Scraping
Create admin panel to:
- Trigger scraping manually
- View scraping history
- Configure scraping schedules
- Monitor scraping status

### 4. Improve Error Messages
When no jobs found, show helpful message:
```typescript
if (jobs.length === 0) {
  return (
    <div>
      <p>No jobs found yet.</p>
      <p>Jobs are updated daily. Please check back tomorrow.</p>
      <Button onClick={triggerScraping}>
        Refresh Jobs Now
      </Button>
    </div>
  )
}
```

## Summary

**Problem**: No jobs in database because scrapers haven't been triggered
**Solution**: Run Remotive scraper (works without API key)
**Alternative**: Fix `.env` typo and use SerpAPI + Remotive
**Long-term**: Implement LinkedIn/Indeed scrapers, add periodic scraping

## Quick Fix Script

Here's a one-command fix:

```bash
cd /home/grejoy/Projects/AI-Powered-JobHunt-Pro/backend

# Create and run seed script
cat > seed_jobs_quick.py << 'EOF'
import asyncio
from app.scrapers.remotive_scraper import RemotiveScraper

async def main():
    scraper = RemotiveScraper()
    jobs = await scraper.scrape(
        keywords=["python", "developer"],
        location="remote",
        max_results=100
    )
    print(f"‚úÖ Found {len(jobs)} jobs from Remotive")
    print("First job:", jobs[0] if jobs else "None")
    return jobs

if __name__ == "__main__":
    asyncio.run(main())
EOF

python seed_jobs_quick.py
```

This will verify the scraper works and show you the jobs being fetched.
